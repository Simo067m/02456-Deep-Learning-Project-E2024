{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 74 - Project 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for reproducing main results\n",
    "### This notebook trains the 2LSTM_Attention model for N epochs, base being 2 epochs.\n",
    "This notebook requires having downloaded the synthetic data \"golf_spectograms_tensor.pt\" and it being placed in the same directory as this notebook. This synthetic data is used due to the original data being unavailable for public sharing. <br>\n",
    "Since the data is synthetic, the train and test data is the same, as it is not intended to show the actual model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from typing import Iterable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Module\n",
    "from torchvision.transforms.functional import resize\n",
    "from torchvision.transforms import transforms\n",
    "from torch.nn.functional import mse_loss as torch_mse_loss\n",
    "from numpy import log10\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, global constants are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2 # the model converges in test perfermance after ~250-300 epochs\n",
    "LEARNING_RATE = 10**-4\n",
    "WEIGHT_DECAY = 10**-3\n",
    "BATCH_SIZE = 10\n",
    "NUM_WORKERS = 10\n",
    "OPTIMIZER = torch.optim.Adam\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "NFFT = 512\n",
    "TS_CROPTWIDTH = (-150, 200)\n",
    "VR_CROPTWIDTH = (-60, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below, some necessary functions for data pre-processing and prediction are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_complexity(model: Module, return_params = False) -> None:\n",
    "    \"\"\"Check and print the number of parameters in the network\n",
    "\n",
    "    Args:\n",
    "        model (module): Pytorch model class\n",
    "    \"\"\"\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model().parameters())\n",
    "    \n",
    "    print(f\"Number of parameters in model {model.__name__}: {total_params} = {'{:.2e}'.format(total_params)}\")\n",
    "\n",
    "def mse_loss(output: torch.Tensor,\n",
    "             target: torch.Tensor) -> torch.Tensor:\n",
    "    return torch_mse_loss(output, target)\n",
    "\n",
    "def weights_init_uniform_rule(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # get the number of the inputs\n",
    "        n = m.in_features\n",
    "        y = 1.0/n**.5\n",
    "        m.weight.data.uniform_(-y, y)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def train_one_epoch(loss_fn, model, train_data_loader, optimizer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    total_loss = 0.\n",
    "\n",
    "    for i, (data, target) in enumerate(train_data_loader):\n",
    "        spectrogram = data.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(spectrogram)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs.squeeze(), target)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        if i % train_data_loader.batch_size == train_data_loader.batch_size - 1:\n",
    "            last_loss = running_loss / train_data_loader.batch_size # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "\n",
    "    return total_loss / (i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below, necessary classes are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class for the synthetic dataset\n",
    "    \"\"\" \n",
    "    def __init__(self, data: torch.Tensor, data_size: int):\n",
    "        self.data = data.permute(2, 0, 1).unsqueeze(0).repeat(data_size, 1, 1, 1)\n",
    "        self.target = torch.rand(data_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.target[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM2_attention(nn.Module):\n",
    "    loss_fn = mse_loss\n",
    "    dataset = SpectrogramDataset\n",
    "    \n",
    "    def __init__(self, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN Feature Extractor\n",
    "        self.cnn_features = nn.Sequential(\n",
    "            # First Conv Block\n",
    "            nn.Conv2d(in_channels=6, out_channels=48, kernel_size=5, stride=2),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "            \n",
    "            # Second Conv Block with Residual Connection\n",
    "            nn.Conv2d(in_channels=48, out_channels=48, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(in_channels=48, out_channels=48, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "            \n",
    "            # Third Conv Block\n",
    "            nn.Conv2d(in_channels=48, out_channels=96, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.AdaptiveAvgPool2d((8, 25))\n",
    "        )\n",
    "        \n",
    "        self.lstm_input_size = 96 * 25\n",
    "        \n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_input_size,\n",
    "            hidden_size=192, \n",
    "            num_layers=3,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(384, 96),  # 384 from bidirectional LSTM (192*2)\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(96, 1)\n",
    "        )\n",
    "        \n",
    "        # Regression Head\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(384, 128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(dropout_rate/2),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "    def attention_net(self, lstm_output):\n",
    "        attention_weights = self.attention(lstm_output)\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "        context = torch.sum(attention_weights * lstm_output, dim=1)\n",
    "        return context\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # CNN Feature Extraction\n",
    "        cnn_out = self.cnn_features(x)\n",
    "        \n",
    "        # Reshape for LSTM\n",
    "        cnn_out = cnn_out.permute(0, 2, 1, 3)  # [batch, height, channels, width]\n",
    "        cnn_out = cnn_out.reshape(batch_size, 8, self.lstm_input_size)  # [batch, seq_len, features]\n",
    "        \n",
    "        # LSTM processing\n",
    "        lstm_out, _ = self.lstm(cnn_out)\n",
    "        \n",
    "        # Apply attention\n",
    "        context = self.attention_net(lstm_out)\n",
    "        \n",
    "        # Final regression\n",
    "        return self.regressor(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "EPOCH 1:\n",
      "  batch 10 loss: 0.2696193292737007\n",
      "  batch 20 loss: 0.2471896454691887\n",
      "  batch 30 loss: 0.10568164624273776\n",
      "  batch 40 loss: 0.10395607501268386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssjsi\\AppData\\Local\\Temp\\ipykernel_10092\\664883625.py:14: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return torch_mse_loss(output, target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.18161167399957776 ; LOSS test 0.07967961579561234\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 0.08612769506871701\n",
      "  batch 20 loss: 0.08793904446065426\n",
      "  batch 30 loss: 0.09118521362543106\n",
      "  batch 40 loss: 0.08546863384544849\n",
      "LOSS train 0.0876801467500627 ; LOSS test 0.08101930469274521\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"golf_spectrograms_tensor.pt\"\n",
    "data_tensor = torch.load(data_dir, map_location=DEVICE, weights_only=True)\n",
    "MODEL = LSTM2_attention\n",
    "\n",
    "print(f\"Using {DEVICE} device\")\n",
    "\n",
    "dataset_train = SpectrogramDataset(data_tensor, data_size=400)\n",
    "dataset_test = SpectrogramDataset(data_tensor, data_size=100)\n",
    "\n",
    "train_data_loader = DataLoader(dataset_train, \n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                shuffle=True,\n",
    "                                num_workers=0)\n",
    "test_data_loader = DataLoader(dataset_test,\n",
    "                                batch_size=3,\n",
    "                                shuffle=False,\n",
    "                                num_workers=0)\n",
    "\n",
    "model = MODEL().to(DEVICE)\n",
    "model.apply(weights_init_uniform_rule)\n",
    "\n",
    "optimizer = OPTIMIZER(model.parameters(), lr=LEARNING_RATE)\n",
    "model_name = f\"model_{MODEL.__name__}\"\n",
    "\n",
    " ## TRAINING LOOP\n",
    "epoch_number = 0\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "        print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "        # Make sure gradient tracking is on\n",
    "        model.train(True)\n",
    "\n",
    "        # Do a pass over the training data and get the average training MSE loss\n",
    "        avg_loss = train_one_epoch(MODEL.loss_fn, model, train_data_loader, optimizer)\n",
    "        \n",
    "        # Calculate the root mean squared error: This gives\n",
    "        # us the opportunity to evaluate the loss as an error\n",
    "        # in natural units of the ball velocity (m/s)\n",
    "        rmse = avg_loss**(1/2)\n",
    "\n",
    "        # Take the log as well for easier tracking of the\n",
    "        # development of the loss.\n",
    "        log_rmse = log10(rmse)\n",
    "\n",
    "        # Reset test loss\n",
    "        running_test_loss = 0.\n",
    "\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Disable gradient computation and evaluate the test data\n",
    "        with torch.no_grad():\n",
    "            for i, (data, target) in enumerate(test_data_loader):\n",
    "                # Get data and targets\n",
    "                spectrogram = data.to(DEVICE)\n",
    "                target = target.to(DEVICE)\n",
    "                \n",
    "                # Get model outputs\n",
    "                test_outputs = model(spectrogram)\n",
    "\n",
    "                # Calculate the loss\n",
    "                test_loss = MODEL.loss_fn(test_outputs.squeeze(), target)\n",
    "\n",
    "                # Add loss to runnings loss\n",
    "                running_test_loss += test_loss\n",
    "\n",
    "        # Calculate average test loss\n",
    "        avg_test_loss = running_test_loss / (i + 1)\n",
    "\n",
    "        # Calculate the RSE for the training predictions\n",
    "        test_rmse = avg_test_loss**(1/2)\n",
    "\n",
    "        # Take the log as well for visualisation\n",
    "        log_test_rmse = torch.log10(test_rmse)\n",
    "\n",
    "        print('LOSS train {} ; LOSS test {}'.format(avg_loss, avg_test_loss))\n",
    "\n",
    "        if avg_test_loss < best_vloss:\n",
    "            best_vloss = avg_test_loss\n",
    "            torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "        epoch_number += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
